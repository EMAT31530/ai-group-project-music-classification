{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"HPO.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNQ6mt9H3Rp8L9jjtS03xZj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"dh9ybzpeJ65K"},"source":["## Hyperparameter optimization\n","Aim: To optimize the efficacy of the dropout layer in our model, by tuning the dropout rate parameter using Bayesian hyperparameter optimization with the `hyperopt` library. We can also tune other hyperparameters of the model such as the learning rate, number of neurons per layer, batch size, etc.\n","Parameters that define the models architechture are called hyperparameters.\n","\n","General method:\n","- Define the range of possible values for the hyperparameter you want to optimize\n","- Define a method for sampling hyperparameter values\n","- Define a metric to evaluate the performance of the model, in our case we will use the validation loss\n","- Define a cross-validation method\n","\n","Bayesian optimization uses a gaussian process to model the prior probablitly of model efficacy across the hyperparameter space, basically approximating how well the model will perform under a certain valued hyperparameter.\n","\n","### Bayesian Optimization\n","Bayesian Optimization is reffered to as a Sequential model-based optimizer (SMBO) algorithm.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7n06wXZ2vmE9","executionInfo":{"status":"ok","timestamp":1617215165258,"user_tz":-60,"elapsed":4350,"user":{"displayName":"Jayvier Pereira","photoUrl":"","userId":"01072379060676764384"}},"outputId":"e53fe427-5eff-4ffc-d9b0-ae56498982e0"},"source":["pip install hyperopt"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: hyperopt in /usr/local/lib/python3.7/dist-packages (0.1.2)\n","Requirement already satisfied: pymongo in /usr/local/lib/python3.7/dist-packages (from hyperopt) (3.11.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from hyperopt) (2.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from hyperopt) (1.4.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from hyperopt) (4.41.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from hyperopt) (1.15.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from hyperopt) (1.19.5)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from hyperopt) (0.16.0)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from networkx->hyperopt) (4.4.2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nHSgByT3gq04","executionInfo":{"status":"ok","timestamp":1617455409777,"user_tz":-60,"elapsed":491,"user":{"displayName":"Jayvier Pereira","photoUrl":"","userId":"01072379060676764384"}}},"source":["import tensorflow as tf\n","import numpy as np\n","import hyperopt\n","from hyperopt import hp, fmin, tpe, Trials\n","from keras.layers import (Input, Conv2D, MaxPooling2D, BatchNormalization)\n","import os"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-bhVsSVtwXlF"},"source":["The hyperparameter space defines all hyperparameters we are going to tune for the model and their accepted range of values. In this case we are going to focus on the following hyperparameters; learning rate, dropout rate, batch normalization, batch size, pooling type, "]},{"cell_type":"code","metadata":{"id":"XNNVp3s7VP0R","executionInfo":{"status":"ok","timestamp":1617445082890,"user_tz":-60,"elapsed":846,"user":{"displayName":"Jayvier Pereira","photoUrl":"","userId":"01072379060676764384"}}},"source":["space = {\n","    # Uniform distribustion to find appropriate learning rate\n","    'lr' : hp.uniform('lr',0.0,1.0),\n","    # Uniform distribustion to find appropriate dropout rate\n","    'dr' : hp.uniform('dr',0.0,0.5),\n","    # To find the best optimizer\n","    'optimizer' : hp.choice('optimizer', ['adam', 'Nadam'])\n","    # Uniform distribustion to find batch size\n","    'batch_size' : hp.uniform()\n","\n","}"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d-KIl3GUdB7S","executionInfo":{"status":"ok","timestamp":1617366287901,"user_tz":-60,"elapsed":430,"user":{"displayName":"Jayvier Pereira","photoUrl":"","userId":"01072379060676764384"}},"outputId":"0ced73f6-74ee-4a23-c69b-d4b448379d81"},"source":["# prints graph of random values for each hyperparameter\n","# These values are not based on any specific model yet\n","print(hyperopt.pyll.stochastic.sample(space))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'dr': 0.19436712036448606, 'lr': 0.7366261700596459, 'optimizer': 'Nadam'}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gLaYbwOOWEDI"},"source":["The fmin function is used to find the optimal value for a scalar valued function.\n","\n","First we define a function for fmin to optimize. In our case we will judge the models performance based on the validation accuracy so we want out optimize function to return the negative value of the accuracy."]},{"cell_type":"code","metadata":{"id":"yiGFcISKnQKA"},"source":["def optimize(hype_space):\n","  \"\"\"\n","  build and train the model based on hype_space parameters\n","  return the negative of the max validation loss for that model\n","  \"\"\"\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":232},"id":"lu1po9yhuznz","executionInfo":{"status":"error","timestamp":1617372036802,"user_tz":-60,"elapsed":666,"user":{"displayName":"Jayvier Pereira","photoUrl":"","userId":"01072379060676764384"}},"outputId":"ca29ea46-a202-4f22-a3c1-85748ac0243b"},"source":["trials = Trials()\n","best = fmin(optimize,\n","            space=space,\n","            algo=tpe.suggest(),\n","            trails=trails,\n","            max_evals=15,\n","            )"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-230d525a8a25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m best = fmin(optimize,\n\u001b[0m\u001b[1;32m      2\u001b[0m             \u001b[0mspace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m             \u001b[0malgo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0mmax_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             )\n","\u001b[0;31mNameError\u001b[0m: name 'optimize' is not defined"]}]}]}